{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse files into single csv file\n",
    "\n",
    "# labels = {'pos':1, 'neg':0}\n",
    "# df = pd.DataFrame()\n",
    "# for s in ('test', 'train'):\n",
    "#     for l in ('pos', 'neg'):\n",
    "#         path ='../../Downloads/aclImdb/%s/%s' % (s, l)\n",
    "#         for file in os.listdir(path):\n",
    "#             with open(os.path.join(path, file), 'r') as infile:\n",
    "#                 txt = infile.read()\n",
    "#                 df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "# df.columns= ['review', 'sentiment']\n",
    "\n",
    "# np.random.seed(0)\n",
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "# df.to_csv('./movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The premise of this movie, of a comedian talk ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I first remember bumping into this zaniness fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I saw this movie without knowing ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  The premise of this movie, of a comedian talk ...          0\n",
       "1  I first remember bumping into this zaniness fr...          1\n",
       "2  First of all I saw this movie without knowing ...          1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./movie_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "['and', 'is', 'shining', 'sun', 'sweet', 'the', 'weather']\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer() # ngram_range(min_kgrams, max_kgrams); (1,1) by default\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and the weather is sweet'\n",
    "])\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)\n",
    "print(count.get_feature_names())\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.43 0.   0.   0.56 0.43 0.56]\n",
      " [0.4  0.48 0.31 0.31 0.31 0.48 0.31]]\n",
      "['and', 'is', 'shining', 'sun', 'sweet', 'the', 'weather']\n"
     ]
    }
   ],
   "source": [
    "# tf-idf: increase weight of low freq words, and decrease weight of high freq words\n",
    "\n",
    "# actual equation implemented by sklearn:\n",
    "# tf = word_occurence_count_in_curr_doc\n",
    "# df = ln(1+doc_n / 1+ docs_n_containing_word)\n",
    "# tf-idf = tf * (df + 1)\n",
    "# tf-idf arr of curr docgoes through N2-normalization (not related to N2 regularization), meaning standardized by dividing by sum of squares\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())\n",
    "print(count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available 1 10  :) =( ;P'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning text data\n",
    "\n",
    "import re # regex\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # <(open bracket) (any char except >)* >(close bracket)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # eyes(:/;/=) optional nose(-) mouth( ) / ( /D/P)\n",
    "    text = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', text)\n",
    "    text = re.sub('\\W+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '') # remove non words(\\W), add back emoticons\n",
    "    return text\n",
    "\n",
    "preprocessor('is seven.<br /><br />Title (Brazil): Not Available :-) =( ;P 1/10!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The premise of this movie, of a comedian talk show host running for president as an independent just to shake things up, is funny, entertaining, brilliant and even a bit inspiring. (thought about the west wing debate when Tom Dobbs leaves his podium, thought about Steven Colbert announcing his candidacy, good times) The first 15 - 20 minutes of this movie are therefore very very entertaining, the debate especially. When he eventually get's elected, it's a pity that is because of a computer glitch, you'd want him to win fair (although that is unrealistic).<br /><br />But after that this movie goes completely downhill. I thought we'd get a great movie like 'Dave' (1993) in which we see how it would out if a comedian actually ran the country. Instead, the movie turns from comedy into a thriller, a romantic comedy and a drama and does none good. The computer glitch becomes the main storyline, which really sucks. Boy is this disappointing. I give it 3 stars just for the premise and because I actually managed to watch this movie from start to end without stopping it, which is usually a good thing with me.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer/Word Splitter\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Stemming (using the Porter Stemmer) - similar to lemmatization, but may created non-words\n",
    "# Porter Stemmer: https://tartarus.org/martin/PorterStemmer/def.txt\n",
    "# Note: in practice, stemming and lemmatization have little impact on the performance of text classification \n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stop Words (extremely common words, e.g. 'is', 'and')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['review'].values\n",
    "y = df['sentiment'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's...nalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0], 'vect__use_idf': [False], 'vect__norm': [None]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Warning: takes a LONG time to finish, especially tokenizer_porter; it would probably improve performance to store the porterized data into a variable and run 2 Grid Searches instead\n",
    "# Note: I skipped testing for tokenizer_portal to save time\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(random_state=0))\n",
    "])\n",
    "\n",
    "# for the 2 hashes, aim to test either use_idf + normalization OR not use_idf + no norm; but not crisscrossing\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer], #[tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [1.0, 10.0, 100.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer], #[tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': [1.0, 10.0, 100.0],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "    }\n",
    "]\n",
    "\n",
    "# grid search - logistic regresion - \n",
    "gs_lr_tfidf = GridSearchCV(\n",
    "    lr_tfidf, \n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5, \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x115db49d8>} \n",
      "Best parameter set: 0.8958 \n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_score_)\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "\n",
    "# Best grid search results using:\n",
    "# - regular tokenizer without Porter stemming, \n",
    "# - no stop-word library \n",
    "# - with tf-idfs & word normalization\n",
    "# - logistic regression classifier that uses L2 regularization \n",
    "# - regularization strength C=10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To save time without running GridSearch again:\n",
    "# best lr_tfidf\n",
    "clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None, ngram_range=(1,1), stop_words=None, tokenizer=tokenizer)),\n",
    "    ('clf', LogisticRegression(random_state=0, penalty='l2', C=10.0))\n",
    "])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CV accuracy: 0.896 +/- 0.004\n",
      "Test CV accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(estimator=clf,\n",
    "                        X=X_train,\n",
    "                        y=y_train,\n",
    "                        cv=5,\n",
    "                        n_jobs=1)\n",
    "print('Training CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n",
    "print('Test CV accuracy: %.3f' % (clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online algorithm\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# combines preprocesser with tokenizer\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # <(open bracket) (any char except >)* >(close bracket)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # eyes(:/;/=) optional nose(-) mouth( ) / ( /D/P)\n",
    "    text = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', text)\n",
    "    text = re.sub('\\W+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', '') # remove non words(\\W), add back emoticons\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# creates a generator, i.e. read one line every time its called\n",
    "# see: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2]) # line is 'review-label-\\n' line[-2] thus retrieves the label\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"The premise of this movie, of a comedian talk show host running for president as an independent just to shake things up, is funny, entertaining, brilliant and even a bit inspiring. (thought about the west wing debate when Tom Dobbs leaves his podium, thought about Steven Colbert announcing his candidacy, good times) The first 15 - 20 minutes of this movie are therefore very very entertaining, the debate especially. When he eventually get\\'s elected, it\\'s a pity that is because of a computer glitch, you\\'d want him to win fair (although that is unrealistic).<br /><br />But after that this movie goes completely downhill. I thought we\\'d get a great movie like \\'Dave\\' (1993) in which we see how it would out if a comedian actually ran the country. Instead, the movie turns from comedy into a thriller, a romantic comedy and a drama and does none good. The computer glitch becomes the main storyline, which really sucks. Boy is this disappointing. I give it 3 stars just for the premise and because I actually managed to watch this movie from start to end without stopping it, which is usually a good thing with me.\"',\n",
       " 0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = stream_docs(path='./movie_data.csv')\n",
    "next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingVectorizer does not require holding all text in memory\n",
    "# Stochastic gradient descent considers only 1 random point while changing weights \n",
    "# unlike gradient descent which considers the whole training data. \n",
    "# As such SGD is much faster than gradient descent when dealing with large data sets\n",
    "# https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(\n",
    "    decode_error='ignore',\n",
    "    n_features=2**21,\n",
    "    preprocessor=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='./movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:26\n"
     ]
    }
   ],
   "source": [
    "import pyprind # for progress bar\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes= np.array([0,1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.869\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
